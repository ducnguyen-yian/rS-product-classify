{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Product Image Classifier\n",
    "\n",
    "This document details the entire process of creating the labeled dataset, defining and training the model, and deploying the results to a SageMaker endpoint.\n",
    "\n",
    "### Quick Warning\n",
    "\n",
    "Most of these collation/creation notebooks are _desctructive_, meaning they will overwrite existing content. You can generate a completely different working dataset in a nondestrutive way by simply changing the `DATASET_NAME` property in `config/test_config.py` or by making a copy and changing that same property and updating the notebooks you're running to point to this new config explicitly. This is obviously not ideal, but it's the case for now.\n",
    "\n",
    "# Table of contents\n",
    "1. [Collecting the product data](#1)\n",
    "    1. [Crawl solution: Query Redshift](#1.1)\n",
    "    1. [Crawl solution: Collating product data by hitting the product service directly](#1.2) — [[notebook](notebooks/collate-products.ipynb)]\n",
    "2. [Preparing to run a Ground Truth Labeling Job](#2)\n",
    "    1. [Creating the manifest file](#2.1) — [[notebook](notebooks/create-ground-truth-manifest.ipynb)]\n",
    "    1. [Uploading the poduct images to S3](#2.2) — [[notebook](notebooks/write-product-images-to-s3.ipynb)]\n",
    "3. [Creating the Ground Truth labeling job](#3)\n",
    "    1. [Category Taxonomy](#3.1) — [[notebook](notebooks/generate-taxonomy.ipynb)]\n",
    "    1. [Labeling Tool Template](#3.2)\n",
    "        1. [Rendering values into HTML](#3.2.1)\n",
    "        1. [Developing the template](#3.2.2) — [[notebook](notebooks/create-labeling-tool.ipynb)]\n",
    "    1. [Pre-annotation Lambda](#3.3)\n",
    "    1. [Post-annotation Lambda](#3.4)\n",
    "    1. [Creating the Labeling Job](#3.5)\n",
    "4. [Evaluating Ground Truth Performance](#4)\n",
    "    1. [Performance metrics](4.1)\n",
    "    1. [Running the same test at 3 different price per tasks](4.2) — [[notebook](notebooks/evaluating-labeling-performance.ipynb)]\n",
    "    1. [Reduced taxonomy and uncropped product photos](4.3)\n",
    "    1. [Sanity check on new changes](4.4)\n",
    "    1. [What to expect](4.5)\n",
    "\n",
    "<a name=\"1\"></a>\n",
    "## 1. Collecting the product data\n",
    "\n",
    "Our dataset will include the product image from the brand/retailer's site, the title, description, and price. We want to create a dataset that's reflective of actual content our influencers post, so we start by pulling products that have been posted in an LTK. Because there exists a spamming problem (influencers will post many often irrelevant products), we choose to only work with first products as a proxy for \"actually relevant\".\n",
    "\n",
    "<a name=\"1.1\"></a>\n",
    "### 1.1 Crawl solution: Query Redshift\n",
    "\n",
    "We can pull this data by hitting Redshift directly:\n",
    "``` mysql\n",
    "select l.id, p.product_id, p.image_url, l.profile_id, l.date_published\n",
    "from ltk_ltks l\n",
    "inner join ltk_ltk_products p\n",
    "\ton l.id = p.ltk_id\n",
    "where l.status = 2\n",
    "and extract(year from l.date_published)=2020\n",
    "and p.position=0\n",
    "order by l.id\n",
    "```\n",
    "\n",
    "I currently just run this on Periscope in [this dashboard](https://app.periscopedata.com/app/rewardstyle/737156/ltk-product-pulls). Eventually the plan is to pull this—and all—data from the data lake, but for now this gets manually stored on S3 [here](s3://data-science-product-image/collation/products.csv).\n",
    "\n",
    "<a name=\"1.2\"></a>\n",
    "### 1.2 Crawl solution: Collating product data by hitting the product service directly\n",
    "\n",
    "Next, we take our list of IDs from S3 and pull the associated product data from the product service. I have to run this locally because the VPC product service is on is not accessible from this role. The collation and writing back to S3 happens in [this notebook](./notebooks/collate-products.ipynb).\n",
    "\n",
    "<a name=\"2\"></a>\n",
    "## 2. Preparing to run a Ground Truth labeling job\n",
    "\n",
    "Next, we have to take the product data we've collated and create everything necessary for Ground Truth to run a labeling job, so basically just an unlabeled dataset.\n",
    "\n",
    "<a name=\"2.1\"></a>\n",
    "### 2.1 Creating the manifest file\n",
    "Ground Truth uses a JSON lines \"manifest\" to serve as an index of every product we want labeled. So all we do is pull down our collated data and generate the file in the expected format with only the fields we're interested in using for labeling. This process occurs in [this notebook](./notebooks/create-ground-truth-manifest.ipynb).\n",
    "\n",
    "<a name=\"2.2\"></a>\n",
    "### 2.2 Uploading the product images to S3\n",
    "The last step in our labeling prep work is to store all our product images on S3. This process happens in [this notebook](./notebooks/write-product-images-to-s3.ipynb)\n",
    "\n",
    "<a name=\"3\"></a>\n",
    "## 3. Creating the Ground Truth labeling job\n",
    "Now that we have our manifest file and all the images on S3, we can create the Ground Truth components used in our labeling job.\n",
    "\n",
    "<a name=\"3.1\"></a>\n",
    "### 3.1 Category Taxonomy\n",
    "I pulled the list of categories from the [product team's category work](https://docs.google.com/spreadsheets/d/1WtKqCdNpncA9744qQSiDZs7j6GEaPmA2xH4qPONMRPQ/edit#gid=896054480), and generated a JSON representation for easy use in the labeling tool, because we need to allow labelers to drill down into categories and back. The taxonomy can be found [here](./files/taxonomy.json). The format is as follows:\n",
    "\n",
    "``` json\n",
    "[\n",
    "    { \"id\": <id1>, \"name\": <name1>, \"subcategories\": [<subcat1>, <subcat2>...], \"parent\": <parent_id> },\n",
    "    { \"id\": <id2>, \"name\": <name2>, \"subcategories\": [<subcat1>, <subcat2>...], \"parent\": <parent_id> },\n",
    "    ...\n",
    "]\n",
    "```\n",
    "\n",
    "If there is no parent (only the root category \"Main\"), the field is not included on the JSON object, and if there are no subcategories, it is defined as an empty list. The taxonomy generation occurs in [this notebook](./notebooks/generate-taxonomy.ipynb).\n",
    "\n",
    "<a name=\"3.2\"></a>\n",
    "### 3.2 Labeling Tool Template\n",
    "Because our labeling is more complex than just image -> label, we need to make a custom template for the Ground Truth labeling job. On the backend, Ground Truth uses a templating system to render values into placeholders in your template, so first we have to decide what data we want to display.\n",
    "\n",
    "<a name=\"3.2.1\"></a>\n",
    "#### 3.2.1 Rendering values into HTML\n",
    "We need to display the product image, title and description, and we also need to display the category selection tree. The variables come attached to the `task.input` object. For example, here is the image, title and description placeholders for a product in the labeling tool template:\n",
    "\n",
    "``` html\n",
    "  <div class=\"left\">\n",
    "    <h3>Product Details:</h3>\n",
    "    <crowd-card>\n",
    "      <div class=\"card\">\n",
    "        <h4 style=\"padding-bottom: 5px;\">Image</h4>\n",
    "        <img src=\"{{ task.input.image | grant_read_access }}\" />\n",
    "\n",
    "        <h4>Title</h4>\n",
    "        <p>{{ task.input.title }}</p>\n",
    "\n",
    "        <h4>Description</h4>\n",
    "        <p>{{ task.input.description }}</p>\n",
    "      </div>\n",
    "    </crowd-card>\n",
    "  </div>\n",
    "```\n",
    "\n",
    "The \"piping\" to `grant_read_access` of the image is a method of converting the s3 path to a one-time image URL. More information on customizing Ground Truth templates can be found [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sms-custom-templates-step2.html).\n",
    "\n",
    "<a name=\"3.2.2\"></a>\n",
    "#### 3.2.2 Developing the template\n",
    "I used [this notebook](./notebooks/create-labeling-tool.ipynb) to work on creating the template. It just mocks the template stuff so you don't have to try to set that up locally and allows you to iterate on the design without leaving SageMaker. The template itself is [here](./files/product-classifier.liquid.html).\n",
    "\n",
    "<a name=\"3.3\"></a>\n",
    "### 3.3 Pre-annotation Lambda\n",
    "Another requirement of the Ground Truth \"custom workflow\" is that you manually process each manifest row before being passed to the templating engine. In our case, I just added all the values we want to render in our labeling tool. The pre-annotation lambda is named `sagemaker-ground-truth-product-labeling-pre-annotation`. The code is very simple:\n",
    "\n",
    "``` python\n",
    "import json\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    manifest_row = {}\n",
    "    entry = event['dataObject']\n",
    "    manifest_row['title'] = entry['title']\n",
    "    manifest_row['description'] = entry['description']\n",
    "    manifest_row['image'] = entry['source-ref']\n",
    "    manifest_row['category_tree'] = <taxonomy JSON>\n",
    "\n",
    "    return {\n",
    "        \"taskInput\": manifest_row\n",
    "    }\n",
    "```\n",
    "\n",
    "<a name=\"3.4\"></a>\n",
    "### 3.4 Post-annotation Lambda\n",
    "Similarly, we have to handle \"consolidating\" the labeling. When we create a job, we select the number of unique workers that label a given product. When those results are passed to our post-annotation lambda, we have to decide what to do with the potentially nonunanimous labels. If you don't use a custom workflow, AWS offers methods for how the winning label is selected and confidence is calculated. I decided to skirt this problem for now and provide full information in the output manifest by calculating confidence as `max_label_count/total_annotations_per_product` and including all the individual workers' respoonses. We can therefore decide to change how we choose a winning label (or discard it altogether) in the future. \n",
    "\n",
    "The lambda is called `sagemaker-ground-truth-product-labeling-post-annotation`, and here is the code:\n",
    "\n",
    "``` python\n",
    "import json\n",
    "import boto3\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def consolidate(labelAttributeName, dataset, threshold=0.5):\n",
    "    label_counts = {}\n",
    "    workers = []\n",
    "    max_count = 0\n",
    "    max_label = None\n",
    "    for annotation_json in dataset['annotations']:\n",
    "        annotation = json.loads(annotation_json['annotationData']['content'])\n",
    "        label = annotation[\"product-category\"]\n",
    "        worker = {\n",
    "                'worker_id': annotation_json['workerId'],\n",
    "                'label': label\n",
    "            }\n",
    "        workers.append(worker)\n",
    "        if label not in label_counts:\n",
    "            label_counts[label] = 1\n",
    "        else:\n",
    "            label_counts[label] += 1\n",
    "        if label_counts[label] > max_count:\n",
    "            max_count = label_counts[label]\n",
    "            max_label = label\n",
    "    confidence = max_count / float(len(dataset['annotations']))\n",
    "    return {\n",
    "            'datasetObjectId': dataset['datasetObjectId'],\n",
    "            'consolidatedAnnotation' : {\n",
    "                'content': {\n",
    "                    labelAttributeName: {\n",
    "                        'workers': workers,\n",
    "                        'confidence': confidence,\n",
    "                        'result': {'product-category': max_label},\n",
    "                        'labeledContent': dataset['dataObject']\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            } \n",
    "    return None\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    consolidated_labels = []\n",
    "\n",
    "    parsed_url = urlparse(event['payload']['s3Uri']);\n",
    "    s3 = boto3.client('s3')\n",
    "    textFile = s3.get_object(Bucket = parsed_url.netloc, Key = parsed_url.path[1:])\n",
    "    filecont = textFile['Body'].read()\n",
    "    annotations = json.loads(filecont)\n",
    "    \n",
    "    for dataset in annotations:\n",
    "        annotation = consolidate(event['labelAttributeName'], dataset)\n",
    "        if annotation:\n",
    "            consolidated_labels.append(annotation)\n",
    "\n",
    "    return consolidated_labels\n",
    "```\n",
    "\n",
    "<a name=\"3.5\"></a>\n",
    "### 3.5 Creating the Labeling Job\n",
    "Finally, all of the components are created and we can actually kick off the labeling job.\n",
    "\n",
    "First, create a new labeling job in the SageMaker console, choose a name for the labeling job, manual set up and point to the manifest file for input, and choose the folder the input manifest sits in as the output, and choose the SageMaker execution role. \n",
    "\n",
    "<img src=\"files/specify-job-details-01.png\" width=\"800\">\n",
    "\n",
    "Secondly, choose Custom task type:\n",
    "\n",
    "<img src=\"files/specify-job-details-02.png\" width=\"800\">\n",
    "\n",
    "Next, select Mechanical Turk and the rest of the parameters you want for the job:\n",
    "\n",
    "<img src=\"files/specify-job-details-03.png\" width=\"800\">\n",
    "\n",
    "Lastly, select Custom template, paste in your custom template, and select the pre and post annotation lambdas:\n",
    "\n",
    "<img src=\"files/specify-job-details-04.png\" width=\"800\">\n",
    "\n",
    "<a name=\"4\"></a>\n",
    "## 4. Evaluating Ground Truth performance\n",
    "After we've created a job, we can look at the output. Unfortunately, I cannot find any information on how exactly to format the output data from the annotation consolidation, so getting things to look nice in Ground Truth's already terrible evaluation tools is difficult/impossible. But the label data is in the output manifest, so we can look at it there.\n",
    "\n",
    "One thing worth noting is that the higher the price, the faster the labeling gets done.\n",
    "\n",
    "<a name=\"4.2\"></a>\n",
    "### 4.1 Performance metrics\n",
    "There are 2 things we'll look for in evaluating the performance of the labeling: \n",
    "\n",
    "    1. How accurately do they match a personally labeled version of the dataset?\n",
    "    2. What percentage of labeled rows are usable?\n",
    "\n",
    "We'll use some function of these 2 values, alongside cost to determine the best parameters for labeling.\n",
    "\n",
    "<a name=\"4.2\"></a>\n",
    "### 4.2 Running the same test at 3 different price per tasks\n",
    "To take a first stab at this, I ran a test at 3 different prices, \\\\$0.036, \\\\$0.048 and \\\\$0.060 per task. The dataset size was 100, which I manually labeled as the gold standard. I then compared each of the jobs to the gold standard.\n",
    "\n",
    "Unfortunately this just isn't a lot of data, and the experiments aren't cheap. While results confirm what we expect (that we get better performance at the higher rates), there are just so many factors at play that it wouldn't be right to draw any strong conclusions.\n",
    "\n",
    "The jobs ended with 26, 47, and 35 items with consensus, respectively, 42, 58, and 60 that matched the gold standard, and 22 / 26, 41 / 47, 31 / 35 of items with consensus that match the gold standard. The results were computed in [this notebook](notebooks/evaluating-labeling-performance.ipynb).\n",
    "\n",
    "~I think the smart course of action would be to drastically pare down the category taxonomy, find/nix similar categories etc. We're also using the cropped product images, which might make the task unnecessarily difficult. These will be what I address first when I get back from vacation.~ [Updated](#4.3)\n",
    "\n",
    "That said, both the higher prices get around 90\\% accuracy, which is definitely servicable.\n",
    "\n",
    "<a name=\"4.3\"></a>\n",
    "### 4.3 Reduced taxonomy and uncropped product photos\n",
    "I cut down on the total categories in a way that removed confusing branching, such as `Womens` -> `Clothing` -> `Activewear` -> `Skirts` and `Womens` -> `Clothing` -> `Skirts` by adding `Activewear` as a child to `Skirts` (and all other previous activewear subcategories). This was a purely manual process.\n",
    "\n",
    "I also omitted cropping from the image service request when asking for the product image. This will hopefully avoid some poorly cropped cases.\n",
    "\n",
    "<a name=\"4.4\"></a>\n",
    "### 4.4 Sanity check on new changes\n",
    "After pruning the taxonomy and switching to uncropped product images, I ran 2 sanity checks, both at \\\\$0.048 per task, and the results were better. One had consensus on 59 of the 100 products, the other had consensus on 75. Of the 59 with consensus in the first test, 56 had the same label as the second test, compared to 68 out of 100 for the entire dataset. \n",
    "\n",
    "Of the 75 in the second dataset, 72 match the gold standard, and all but 1 were within the realm of reason. This is a strong signal that consensus is a fair proxy for accuracy.\n",
    "\n",
    "<a name=\"4.5\"></a>\n",
    "### 4.5 What to expect\n",
    "If we conservatively choose the lower performing job where 60\\% of products achieve labeling consensus and we run with 3 independent labelers per product at \\\\$0.048 per task, it will cost us $24,000 to get to the 100K product dataset we'd like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
